# 最近学习进度
## 关于服务器
服务器目前的状态是失联的，而我在折腾这件事上面可以说是失败的。
在7.4日时第一次不带GPU地装机成功，也在上面跑了些程序测试了下，除了满载容易过热以外，没什么大的问题。然后在7.3日订好的转接线（原定于7.4日送达）在7.6日才发货，7.6日晚上9点才收到并测试。虽然在之前已经有过英伟达显卡装驱动的经验，但是还是低估了难度，以至于到目前都还是无法以GUI的方式操作。7.7日电脑搬进机房，并调试失败。7.8日再调试，还是失败。后面高老师大概回家了，管理员可能没联系上或者不在学校。
现在看来这个失败是我过于仓促导致的。我应该更早地购买转接线或者催促商家更早地发货。如果有多一天或者两天有网络的情况下调试可能情况就大有不同。或者在7.8调试失败后将服务器拿出机房，拿到办公室、宿舍或家里继续调试。
## 关于论文
现在手头上只剩下平板了。别说训练或者微调了，1.7b的llm模型推理都几乎不可用。所以目前的状态是做不了的。
## 关于其它的学习内容
工科线性代数基本复习完了。numpy和pandas因为要参加数模也学了，至少是基本能用的程度。另外c语言也复习的差不多了，下星期复习复习其它内容，最晚会在下下个星期参加一生一芯的入学测试。
## 阅读arXiv:2404.04815v1  （Allo: A Programming Model for Composable Accelerator Design于PLDI会议）
### AI总结
创新点
| 类别        | 具体创新                                                                                                   |
| --------- | ------------------------------------------------------------------------------------------------------ |
| **编程模型**  | 提出 **Allo**，首个支持**可组合（composable）**、\*\*渐进式（progressive）\*\*硬件定制的加速器设计语言（ADL），解决现有ADL只优化单核、无法模块化组合的问题。 |
| **解耦机制**  | 将**算法描述**与**硬件定制**（计算、存储、通信、数据类型）完全解耦，封装为可验证的“定制原语”（customization primitives）。                         |
| **可组合调度** | 引入 `.compose()` 原语，支持**自底向上**将多个优化后的子模块（如 systolic array、GEMM）组合为复杂设计，突破传统HLS的扁平化限制。                   |
| **类型系统**  | 提出**基于格（lattice）的类型系统**，用于统一内存布局（如数组分区类型），通过数据流分析实现类型安全的接口匹配。                                          |
| **数据流优化** | 提出**分层数据流图（hierarchical dataflow graph）**，用于跨函数边界的全局优化（如FIFO大小计算），避免HLS中“拼积木”导致的性能损失。                  |
| **工具链**   | 实现端到端编译器：Python前端 → MLIR方言 → HLS C++/LLVM IR，支持PyTorch模型直接导入（如GPT-2），首次在FPGA上完整部署大模型。                  |

数学基础或启发
| 维度        | 具体方法                                                                                    | 数学/理论来源                                         |
| --------- | --------------------------------------------------------------------------------------- | ----------------------------------------------- |
| **类型系统**  | 数组分区的四种类型（⊥, Cₙ, Bₙ, ⊤）构成**完全格（complete lattice）**，通过\*\*子类型关系（subtyping）\*\*解决接口匹配冲突。  | Hindley-Milner类型系统（扩展）、格理论（Knaster-Tarski不动点定理） |
| **数据流分析** | FIFO大小计算建模为**最大缓冲深度问题**，通过生产/消费速率函数求解；全局传播采用**工作列表算法（worklist algorithm）**。             | Kahn进程网络（KPN）、静态数据流分析                           |
| **程序变换**  | 每个定制原语视为**程序重写规则（program rewrite rules）**，确保局部正确性后通过组合实现全局正确性。                          | Exo/TVM的调度语言、Halide的算法-调度解耦思想                   |
| **硬件映射**  | 通过\*\*空间架构（systolic array）**的时空映射，将算法循环嵌套映射为PE阵列，利用**多面体模型（polyhedral model）\*\*分析数据依赖。 | Kung-Leiserson脉动阵列理论、多面体编译技术                    |

不足和未来的研究方向
| 不足类别      | 具体问题                                                 | 未来方向                                           |
| --------- | ---------------------------------------------------- | ---------------------------------------------- |
| **自动化不足** | 需手动设计调度策略（如循环分块、FIFO大小），缺乏**自动调度器（auto-scheduler）**。 | 结合ML（如图神经网络）实现自动调度搜索（类似Ansor）。                 |
| **缓冲管理**  | 当前仅支持FIFO连接，未解决**跨内核的缓冲自动插入**（如双缓冲、非连续访问）。           | 开发\*\*自动缓冲化（auto-bufferization）\*\*技术，处理复杂依赖。  |
| **后端优化**  | 大设计（如GPT-2）在FPGA布线阶段因跨die通信导致频率下降。                   | 集成**布局感知优化**（如AutoBridge），支持多die FPGA的并行编译与链接。 |
| **生态局限**  | 仅支持AMD FPGA（Vitis HLS），需适配其他后端（ASIC、Intel FPGA）。     | 扩展至CIRCT（LLVM电路IR工具链），支持ASIC和更多FPGA平台。         |
| **验证扩展**  | 当前等价性检查依赖静态控制流（SICF），不支持动态shape或参数化设计。               | 引入**符号执行**或**SMT求解器**处理动态行为验证。                 |
### 读后感
之前看过伯克利那篇文章是借助chisel构建AI加速器，这一篇使用python构建，抽象性和自动化程度又得到了提高。可见构建自动化地构建硬件也算是一个趋势（或者说这类研究方向有点像EDA）。这个。和很多其它与AI加速器相关的做法一样，这项研究的做法同样也是将一些循环 
## 阅读arXiv:2312.15159v1（Understanding the Potential of FPGA-Based Spatial Acceleration for Large Language Model Inference于FCCM）
### AI总结
创新点
* 引入了一个分析框架，首次深入分析了基于FPGA的LLM空间加速的优势和局限性。此框架不仅可以估算特定加速器配置在给定FPGA设备上的性能，还为LLM推理加速器的设计提供了指导。
* 创建了一套模块化且可重用的HLS（高级综合）内核套件，专门用于构建不同Transformer模型的FPGA空间加速器。该内核库计划开源，并有望成为衡量HLS和FPGA加速的宝贵资源。
* 利用其内核库，设计并实现了一系列高性能基于FPGA的LLM加速器，其加速效果可与现有GPU和FPGA加速器媲美。具体来说，对于BERT模型，比现有FPGA加速器提速16.1倍。对于GPT生成推理，在prefill阶段比DFX（一种基于FPGA的overlay架构）提速2.2倍，在decode阶段提速1.9倍，能效提高5.7倍。

受的启发或数学基础
* 该研究的灵感来源于Transformer模型在LLMs中的快速发展，以及在推理工作负载中对高效部署的巨大需求。
* 与现有主要依赖时间架构（重用硬件单元）的方法不同，该方法通过为特定操作符或层专门化不同的硬件单元，并通过数据流架构实现它们之间的直接通信，从而最大程度地减少片外内存访问。
* 该研究引入了一个综合的分析模型，用于估算空间LLM加速器的性能，其中考虑了FPGA上可用的片上计算和内存资源。该模型可以扩展到多FPGA设置以进行分布式推理。
* 主要的计算需求基于Transformer模型中的通用矩阵乘法（GEMM或Matmul）或通用矩阵向量乘法（GEMV）操作，并使用乘法累加（MACs）的数量作为量化计算需求的代理指标。内存容量约束和内存端口约束也通过数学公式进行了建模。

不足
* **处理LLMs中多样并行性的挑战：** LLMs的生成推理过程包括同时处理用户提示和按自回归方式顺序生成新令牌两个不同阶段，它们具有显著不同的计算和内存特性。这使得硬件加速器需要针对其特定需求进行定制。大量的参数和中间张量进一步使得片上和片外存储的选择变得复杂。
* **硬件加速器中缺乏标准LLM构建块的挑战：** LLM架构的快速演变与硬件开发相对缓慢的步伐形成对比。尽管在软件领域已经提出了大量的Transformer构建块，但缺少可重用的硬件加速器设计块阻碍了开发进展。现有的许多框架仅限于小型CNN设计，缺乏对复杂Transformer模型的支持，并且难以扩展到大型模型和多芯片FPGA。
* 在prefill阶段，FPGA在计算资源方面表现不如GPU，导致其性能不佳。
* 虽然分析框架能够精确预测性能，但在decode阶段，估算的延迟低于实际结果，这主要是因为两个操作符之间的初始间隔并不显著小于一个阶段的执行时间，导致延迟显著增加。

后续研究方向
* **AI优化型FPGA：** 未来的AI工作负载FPGA应提供足够的内存带宽和高效的片上互连，以促进空间架构中的本地数据移动。关于现有编程实践是否能有效执行Transformer模型仍是一个悬而未决的问题。
* **多芯片FPGA上的时序收敛：** 在多芯片FPGA上，充分探索多芯片分区和扩展的设计空间很困难。尽管有自动化框架生成布局规划约束，但它们目前不足以捕捉Transformer模型中各种数据移动方案（如残差连接、多头拆分）。
* **异构部署：** 鉴于数据中心日益异构化，未来可能需要利用不同硬件的优势来加速Transformer模型，例如GPU适用于GPT的prefill阶段，而FPGA擅长低延迟的decode阶段。构建一个有效管理数百个异构设备的分布式系统是关键挑战。
### 读后感
原来FCCM是CCF B评级，GPT找的时候还给我标成A级了。。。没有很明显的突破或者说创新，但是很直观地整合了一些现有的方案，省去了到处找资料的麻烦。