# 学习进度报告
## 1.阅读 DOI: 10.1109/ICCMC.2019.8819779
### 读后感
对比番茄炒鸡蛋用花生油、大豆油、菜籽油式文章，这里就不拿AI总结了。看这篇东西的目的主要在于集创赛中加法器设计的选择。但是对于这篇文章，我有一个疑惑：RCA的延迟为什么会低于CLA。
## 2.初步了解卷积和快速傅里叶变换和一些数论（可能是？）
其实只了解了相当小的一部分，只看了离散的部分。知道了定义、几何上的直观体现和一些在计算机上的应用。很有意思的一点，乘法可以使用卷积的视角看待。了解完乘法后顺便了解了用FFT（其实并非FFT，只是习惯上这么叫）做快速大数乘法，同时也了解到FFT精度丢失的问题，看到了NTT。由于对数论的部分一窍不通，从头开始看了代数学引论（B.A.卓里奇）。但由于懈怠，也就初步看到了群、环、域那一部分，而且一题都没做。
## 3.AI辅助的学习
大概就是DeepSeek带着看代数学引论。但是实际上效果并不好（可能是提示词给得不好）新建对话后再讲课的方式可以几乎说完全不一样。然后就回去直接对着书啃了，也没怎么用AI了。
## 4.阅读 DOI: 10.3390/s23052401
### AI总结
无ADC/DAC设计：该架构最显著的特点是取消了传统PIM设计中用于数据转换的模拟数字转换器 (ADC) 和数字模拟转换器 (DAC)。  这一改进有效降低了大规模算法下的硬件消耗，特别是功耗和芯片面积。    

消除额外内存需求：通过优化数据流，该架构在卷积计算过程中无需额外的内存存储，从而避免了大量数据传输的瓶颈。    

部分量化技术：为了在提升效率的同时尽可能减少精度损失，论文引入了部分量化方案。  该方案仅对权重进行量化，而特征数据则保持较高精度，从而在保证识别准确率的前提下，大幅减少RRAM存储空间。    
数字域计算：所有计算都在数字域进行，进一步降低了数据转换的成本。    
### 读后感
哇，还有GDUT。这篇提到的RRAM似乎有很大的发展前景，更容易做成存算一体的结构。该架构使用D触发器和纹波计数器将特征转换为脉冲 ，从而无需ADC/DAC。同时利用了，CNN中的卷积操作具有局部连接和权重共享的特点。我认为下一步可能的创新时用在更多类型的神经网络上和改进RRAM（很笼统，但是RRAM各方面的发展确实不大成熟）
## 5.阅读  arXiv:2304.03986v2
### AI总结
专门的硬件加速器设计： SwiftTron 是一个专门的加速器，它由多个硬件单元组成，旨在执行 Transformer 的不同操作，从而实现高性能和高能效。   

整数算术量化方案： SwiftTron 使用一种量化方案，该方案利用缩放因子，以 INT8 算术执行线性运算，并以 INT32 算术执行非线性运算，从而优化计算效率。   

硬件效率： 通过使用全整数运算，SwiftTron 避免了对浮点单元的需求，与传统方法相比，降低了硬件复杂性和资源开销。   
### 读后感
暂时没看完
