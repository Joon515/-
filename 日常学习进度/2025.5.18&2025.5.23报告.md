# 学习进度报告
因为感冒原因，5.18的报告合并至此。
## 1. 阅读arXiv:2106.09685v2
### AI摘要
1.  **低秩适应（Low-Rank Adaptation）：** 这是 LoRA 最主要的创新。它认识到在对预训练大型模型进行微调时，权重的更新可能具有较低的“内在维度”或“有效秩”。因此，与其更新整个高维权重矩阵 $\Delta W$，不如将其分解为两个更小的低秩矩阵的乘积，即 $\Delta W = BA$，其中 $B$ 的维度是 $d \times r$， $A$ 的维度是 $r \times k$，而 $r \ll \min(d, k)$。这样做可以将可训练参数量从 $d \times k$ 显著减少到 $r \times (d+k)$。
2.  **冻结预训练权重，注入增量更新：** LoRA 的方法是冻结预训练模型的原始权重 $W_0$，并且在每个 Transformer 层中（特别是查询和值投影矩阵）注入这些可训练的低秩更新矩阵。这意味着模型的核心知识得以保留，而适应新任务所需的仅仅是这些小幅度的增量调整。
3.  **不引入推理延迟：** LoRA 的设计允许在推理时将学习到的低秩矩阵 $BA$ 与原始的预训练权重 $W_0$ 相加，形成 $W_0' = W_0 + BA$。这意味着在部署阶段，可以完全消除 LoRA 模块，不增加任何额外的计算开销或推理延迟，这是其相比其他参数高效微调方法的显著优势。
4.  **模块化和可切换性：** 由于每个任务只需要一套小的 LoRA 权重，这使得在不同任务之间切换变得极其方便。对于多个下游任务，无需为每个任务存储一个完整的微调模型副本，只需存储一套基础模型和多套小的 LoRA 参量。这极大地降低了存储成本和部署复杂性。
5.  **跨模型和任务的有效性：** 论文通过在 ROBERTa、DeBERTa、GPT-2 和 GPT-3 等多种大型模型以及各种下游任务（如文本分类、文本生成等）上进行实验，证明了 LoRA 方法的通用性和有效性，其性能可以媲美甚至超越全量微调。
### 读后感
见下一篇论文的读后感
## 2. 阅读arXiv:2504.07448v1
### AI摘要
本文介绍了一种名为 LoRI（LoRA with Reduced Interference）的参数高效微调（PEFT）方法，旨在减少多任务场景下的跨任务干扰。
核心思想：
* 固定投影矩阵 A 为随机投影 [cite: 2]。
* 使用任务特定的掩码对矩阵 B 进行稀疏化处理 [cite: 2]。
主要优点：
* 显著减少可训练参数数量，最高可比 LoRA 少 95% [cite: 5, 190]。
* 在保持强大任务性能的同时，减轻了多任务环境下的参数干扰 [cite: 3, 4]。
* 通过利用适配器子空间之间的正交性，最大限度地减少适配器合并中的跨任务干扰 [cite: 4, 32, 33]。
* 通过利用稀疏性减轻灾难性遗忘，支持持续学习 [cite: 4, 38, 100, 101]。
实验结果：
* 在自然语言理解、数学推理、代码生成和安全对齐任务上的广泛实验表明，LoRI 的性能优于全量微调和现有的 PEFT 方法 [cite: 5]。
* 在多任务实验中，LoRI 实现了有效的适配器合并和持续学习，并减少了跨任务干扰 [cite: 6]。
* 即使 B 矩阵的稀疏度达到 90%，LoRI 也能保持稳定的性能 [cite: 16]。
* 在代码生成任务上，LoRI 的表现显著优于 FFT、LoRA 和 DoRA [cite: 130]。
* LoRI-D 在合并适配器时实现了最佳的整体性能，与单任务基线非常接近，表明 LoRI 适配器之间的干扰最小 [cite: 148]。
* 在持续学习中，LoRI 在减轻安全对齐的灾难性遗忘方面显著优于 LoRA [cite: 164, 168]。
方法细节：
* 冻结投影 A 矩阵：LoRI 将 A 矩阵固定为随机投影，只训练 B 矩阵，从而减少内存消耗 [cite: 14, 58, 59]。
* B 矩阵的稀疏掩码：LoRI 通过校准过程提取稀疏掩码，只更新 B 矩阵中最相关的参数 [cite: 61, 62]。掩码根据参数的绝对值大小进行选择，保留最高比例的参数 [cite: 65, 66]。
* 适配器合并：利用 LoRI 适配器在表示空间中的近似正交性来减少参数干扰 [cite: 77, 78, 80, 81]。论文中描述了连接合并（加权平均）和线性合并两种方法 [cite: 83]。连接合并在减少跨任务干扰方面更有效 [cite: 88]。
* 持续学习：采用两阶段持续学习过程来构建安全适配器，首先在安全数据集上训练，然后在每个下游任务上进行微调，同时保留安全对齐 [cite: 93, 94]。B 矩阵的稀疏性有助于隔离参数更新，减轻灾难性遗忘 [cite: 100, 101]。
代码可在以下链接获取：[https://github.com/juzhengz/LoRI]
### 读后感
无论是LoRA还是LoRI，都是通过在冻结原有矩阵的同时对权重矩阵的较小的”有效秩“进行调整。相比起全尺寸微调，这类调整方式减小了调整的参数量。与prfix-tuning、adapter这种“串联”的微调方式相比，LoRA和LoRI采取的是增加的旁路的“并联”微调方式。与串联方式相比，并联的方式不会增加模型的层数，减小了推理延迟。并联方式应该更容易实现软硬件上的并行算法方面的优化。我猜测并联方式更不容易引起遗忘或泛化能力下降的原因是数据流在经过原有模型矩阵时不会被微调部分的矩阵变换。而且并联方式也更更利于多个微调模型的合并。但是如果纯粹地使用并联的方式微调模型，也有一些弊端。在多任务学习中，如果多个任务之间存在大量的共享知识，为每个任务训练一个独立的旁路意味着，即使这些任务有共同的基础，每个旁路也需要“重新学习”这些共通的部分，导致参数冗余和训练效率的降低。根据现有应用，LoRA在秩非常低（激进）的情况下，效果不佳。而LoRI稀疏掩码的生成我尚不能理解。
## 3.阅读DOI:10.1109TCAD.2020.2989373(Song_2020_IEEE_ITT-RNA)
### AI摘要
1.  **提出了ITT-RNA训练方法：** 该方法利用神经网络的“自愈能力”来规避RRAM交叉阵列中的制造缺陷。
2.  **引入了"权重重要性"概念：** 根据权重的大小和其对模型精度的影响，评估不同权重的重要性，并优先保护重要的权重。
3.  **设计了加速器友好的训练流程：** 将训练过程与RRAM交叉阵列的物理缺陷分布相结合，在训练阶段就考虑了硬件缺陷，从而降低了对硬件良品率的要求。
4.  **提出了动态调整机制：** 使得ITT-RNA能够适用于更复杂的DNN模型，如MLP和LeNet-5，通过在训练过程中动态调整权重分配策略来进一步提高缺陷容忍度。
### 读后感
又是一个与RRAM相关的文章。RRAM似乎是神经网络加速器存储器的一个新的方向。若以RRAM代替传统的DRAM，在训练、推理给过程中，RRAM的非易失性可以减小数据加载的频率，减小传输方面的瓶颈，并带来更低的功耗。而且RRAM似乎可以实现矩阵*向量的乘法操作，感觉这会为存算一体提供便利（但专用性可能比较强）。回到这篇文章，ITT-RNA通过训练前的检测，并在训练时通过正则化，避免把重要权重分配到有缺陷的忆阻器上。同时，当有权重被分配到有缺陷的忆阻器时，其余权重也会被调整以弥补。我认为这个做法存在一定缺陷。在训练的每次迭代中，需要额外的计算弥补缺陷忆阻器上的权重，这样可能会显著增加反向传播时间，也可能阻碍收敛。（文章似乎没有列出这种开销的表现）
## 4.其它的一些学习
复习了下systemverilog。同时稍微看了点与神经网络稀疏性相关的内容，但是不成体系。继续看代数学引论。接下来准备复习下工科线性代数，学下python，继续看代数学引论。在代数学引论看差不多的时候系统学习下统计学