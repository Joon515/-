# 学习进度报告
这个星期接近数学考试了，所以看的东西并不多。
## 了解注意力和多头注意力机制
仅仅是了解。我注意到虽然注意力机制的感受野是全局，但是这也会带来极大的时间复杂度和空间复杂度。这种局限在语言模型的体现上就是上下文长度。我觉得可能可以参考图像处理方面的AI的一些做法进行改进。(但暂时还没时间去了解)
## 关于LoRA的进一步“考古”
发现有资料提及LoRA之前的GPT3就通过以矩阵乘积的方式对V矩阵进行低秩压缩以节省存储空间。

